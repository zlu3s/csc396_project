{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Research Emotion Labels\n",
    "//Testing what emotion labels the model outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/cheepheng/miniconda3/envs/csc396/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/cheepheng/miniconda3/envs/csc396/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:1748: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "def get_emotion(text):\n",
    "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt', max_length=512,\n",
    "  truncation=True)\n",
    "\n",
    "  output = model.generate(input_ids=input_ids,\n",
    "               max_length=2)\n",
    "  \n",
    "  dec = [tokenizer.decode(ids) for ids in output]\n",
    "  label = dec[0]\n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lyrics = pd.read_csv('spotify_millsongdata.csv')\n",
    "all_lyrics = lyrics['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing emotion detection on sample texts:\n",
      "============================================================\n",
      "Text: I am so happy and excited about this wonderful day...\n",
      "Emotion: <pad> joy\n",
      "------------------------------------------------------------\n",
      "Text: I feel so sad and lonely, everything is falling ap...\n",
      "Emotion: <pad> sadness\n",
      "------------------------------------------------------------\n",
      "Text: I am furious and angry about what happened!...\n",
      "Emotion: <pad> anger\n",
      "------------------------------------------------------------\n",
      "Text: I am terrified and scared of what might happen....\n",
      "Emotion: <pad> fear\n",
      "------------------------------------------------------------\n",
      "Text: I love you so much, you mean everything to me....\n",
      "Emotion: <pad> love\n",
      "------------------------------------------------------------\n",
      "Text: Wow, I didn't expect that at all! What a surprise!...\n",
      "Emotion: <pad> surprise\n",
      "------------------------------------------------------------\n",
      "Text: I feel so calm and peaceful right now....\n",
      "Emotion: <pad> joy\n",
      "------------------------------------------------------------\n",
      "Text: This is the best thing ever, I'm thrilled!...\n",
      "Emotion: <pad> joy\n",
      "------------------------------------------------------------\n",
      "Text: I'm feeling down and depressed today....\n",
      "Emotion: <pad> sadness\n",
      "------------------------------------------------------------\n",
      "Text: How dare you do this to me! I'm outraged!...\n",
      "Emotion: <pad> anger\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with diverse sample texts to discover all possible emotion labels\n",
    "test_texts = [\n",
    "    \"I am so happy and excited about this wonderful day!\",\n",
    "    \"I feel so sad and lonely, everything is falling apart.\",\n",
    "    \"I am furious and angry about what happened!\",\n",
    "    \"I am terrified and scared of what might happen.\",\n",
    "    \"I love you so much, you mean everything to me.\",\n",
    "    \"Wow, I didn't expect that at all! What a surprise!\",\n",
    "    \"I feel so calm and peaceful right now.\",\n",
    "    \"This is the best thing ever, I'm thrilled!\",\n",
    "    \"I'm feeling down and depressed today.\",\n",
    "    \"How dare you do this to me! I'm outraged!\"\n",
    "]\n",
    "\n",
    "print(\"Testing emotion detection on sample texts:\")\n",
    "print(\"=\" * 60)\n",
    "emotions_found = set()\n",
    "for text in test_texts:\n",
    "    emotion = get_emotion(text)\n",
    "    emotions_found.add(emotion)\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Emotion: {emotion}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All unique emotions detected by the model:\n",
      "['<pad> anger', '<pad> fear', '<pad> joy', '<pad> love', '<pad> sadness', '<pad> surprise']\n"
     ]
    }
   ],
   "source": [
    "# Display all unique emotions found\n",
    "print(\"\\nAll unique emotions detected by the model:\")\n",
    "print(sorted(emotions_found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token IDs for each emotion:\n",
      "joy: tokens=['▁joy'], ids=[3922]\n",
      "sadness: tokens=['▁sadness'], ids=[24784]\n",
      "anger: tokens=['▁anger'], ids=[11213]\n",
      "love: tokens=['▁love'], ids=[333]\n",
      "surprise: tokens=['▁surprise'], ids=[4158]\n",
      "fear: tokens=['▁fear'], ids=[2971]\n"
     ]
    }
   ],
   "source": [
    "# Now let's get the token IDs for each emotion label\n",
    "# First, clean the emotion labels (remove special tokens)\n",
    "clean_emotions = [e.replace('<pad>', '').replace('</s>', '').strip() for e in emotions_found]\n",
    "clean_emotions = [e for e in clean_emotions if e]  # Remove empty strings\n",
    "\n",
    "print(\"\\nToken IDs for each emotion:\")\n",
    "emotion_token_map = {}\n",
    "for emotion in clean_emotions:\n",
    "    tokens = tokenizer.tokenize(emotion)\n",
    "    token_ids = tokenizer.encode(emotion, add_special_tokens=False)\n",
    "    emotion_token_map[emotion] = token_ids\n",
    "    print(f\"{emotion}: tokens={tokens}, ids={token_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing raw model output:\n",
      "Input shape: torch.Size([1, 13])\n",
      "Logits shape: torch.Size([1, 1, 32128])\n",
      "Vocabulary size: 32128\n",
      "\n",
      "Top 10 most likely tokens:\n",
      "joy: 0.9995\n",
      "love: 0.0003\n",
      "happiness: 0.0001\n",
      "surprise: 0.0000\n",
      "Joy: 0.0000\n",
      "sadness: 0.0000\n",
      "delight: 0.0000\n",
      "joyful: 0.0000\n",
      "excitement: 0.0000\n",
      "happy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test getting raw logits instead of generate()\n",
    "import torch\n",
    "\n",
    "test_text = \"I am so happy and excited about this wonderful day!\"\n",
    "input_ids = tokenizer.encode(test_text + '</s>', return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "print(\"Testing raw model output:\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Get model output with logits\n",
    "with torch.no_grad():\n",
    "    # For T5, we need to provide decoder_input_ids\n",
    "    decoder_start_token_id = model.config.decoder_start_token_id\n",
    "    decoder_input_ids = torch.tensor([[decoder_start_token_id]])\n",
    "    \n",
    "    outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Vocabulary size: {model.config.vocab_size}\")\n",
    "    \n",
    "    # Get probabilities using softmax\n",
    "    probs = torch.softmax(logits[0, 0, :], dim=-1)\n",
    "    \n",
    "    # Get top 10 most likely tokens\n",
    "    top_probs, top_indices = torch.topk(probs, k=10)\n",
    "    \n",
    "    print(\"\\nTop 10 most likely tokens:\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx.item()])\n",
    "        print(f\"{token}: {prob.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc396",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
